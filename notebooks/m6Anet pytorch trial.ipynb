{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T06:01:59.816855Z",
     "start_time": "2022-10-14T06:01:59.477417Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T05:06:47.240747Z",
     "start_time": "2022-10-13T05:06:47.237877Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T05:06:58.661269Z",
     "start_time": "2022-10-13T05:06:58.658628Z"
    }
   },
   "outputs": [],
   "source": [
    "data = {\"AAGGA\" : 0,\n",
    "        \"AAGGA\" : 1,\n",
    "        \"AAGGA\" : 2, \n",
    "        \"AAGGT\" : 3, \n",
    "        \"ATGGA\" : 4, }\n",
    "        # . . . 66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T05:07:10.880984Z",
     "start_time": "2022-10-13T05:07:10.875352Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input=torch.tensor([0, 1, 2])\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T05:07:37.887853Z",
     "start_time": "2022-10-13T05:07:37.863868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0235, -1.2632],\n",
       "        [ 0.4791,  0.8433],\n",
       "        [ 2.8003,  2.0574]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = nn.Embedding(66, 2)\n",
    "embeddings(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T05:08:43.817150Z",
     "start_time": "2022-10-13T05:08:43.811454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples = 8, n_features = 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Import the neural network module from pytorch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Linear regression\n",
    "# f = w * x \n",
    "# here : f = 2 * x\n",
    "\n",
    "# 0) Training samples, watch the shape!. Here we have (8, 1) for 8 observations of 1 feature each\n",
    "X = torch.tensor([[1], [2], [3], [4], [5], [6], [7], [8]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8], [10], [12], [14], [16]], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(f'n_samples = {n_samples}, n_features = {n_features}')\n",
    "\n",
    "# 0) create a test sample\n",
    "X_test = torch.tensor([5], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-13T05:08:53.808204Z",
     "start_time": "2022-10-13T05:08:53.745533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5.0) = -2.208\n",
      "epoch  10 : w =  1.9433797597885132  loss =  0.020263902842998505\n",
      "epoch  20 : w =  1.9468997716903687  loss =  0.018501583486795425\n",
      "epoch  30 : w =  1.9489827156066895  loss =  0.017078982666134834\n",
      "epoch  40 : w =  1.9509832859039307  loss =  0.015765808522701263\n",
      "epoch  50 : w =  1.9529054164886475  loss =  0.014553562738001347\n",
      "epoch  60 : w =  1.9547522068023682  loss =  0.013434533029794693\n",
      "epoch  70 : w =  1.956526517868042  loss =  0.012401577085256577\n",
      "epoch  80 : w =  1.9582313299179077  loss =  0.011448007076978683\n",
      "epoch  90 : w =  1.959869146347046  loss =  0.010567796416580677\n",
      "epoch  100 : w =  1.9614428281784058  loss =  0.009755214676260948\n",
      "Prediction after training: f(5.0) = 10.024\n"
     ]
    }
   ],
   "source": [
    "# 1) Design Model, the model has to implement the forward pass!\n",
    "\n",
    "# Here we could simply use a built-in model from PyTorch\n",
    "# model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# Pytorch model class must ALWYAS inherit from nn.module\n",
    "class LinearRegression(nn.Module):\n",
    "    # Must always init the pytorch model class\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Need to super init\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define different layers. Here there is only one linear for the linear regression\n",
    "        # nn.Linear performs the linear regression operation w*x + b\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    # Apply the layers. Need to include x in function signature so model has input\n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "\n",
    "# Specifying the input and output dimensions\n",
    "input_size, output_size = n_features, n_features\n",
    "# Insantiate Linear Regression Neural Network Model\n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "print(f'Prediction before training: f({X_test.item()}) = {model(X_test).item():.3f}')\n",
    "\n",
    "# 2) Define loss and optimizer\n",
    "learning_rate = 0.01\n",
    "n_epochs = 100\n",
    "# Automatically implements the MSE formula\n",
    "loss = nn.MSELoss()\n",
    "# Use Stochastic Gradient Descent. Need to supply the model parameters \n",
    "# and a selected learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    # predict = forward pass with our model\n",
    "    # Internally this calls model.forward(x), performing the linear regression\n",
    "    # and returning the predicted y_values\n",
    "    y_predicted = model(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_predicted)\n",
    "\n",
    "    # calculate gradients = backward pass\n",
    "    l.backward()\n",
    "\n",
    "    # update weights. This updates model.parameters()\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        w, b = model.parameters() # unpack parameters. In this instance the weight & the bias\n",
    "        print('epoch ', epoch+1, ': w = ', w[0][0].item(), ' loss = ', l.item())\n",
    "        \n",
    "\n",
    "print(f'Prediction after training: f({X_test.item()}) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T06:43:35.520498Z",
     "start_time": "2022-10-14T06:43:35.517243Z"
    }
   },
   "outputs": [],
   "source": [
    "# To be able to import the file\n",
    "import sys\n",
    "pathname=\"/Users/carelchay/Desktop/School/Modules/DSA4262/Project 2/DSA4262-frontasticfour/scripts\"\n",
    "if pathname not in sys.path:\n",
    "    sys.path.append(pathname)\n",
    "path_to_data = \"/Users/carelchay/Desktop/School/Modules/DSA4262/Project 2/data/data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T06:43:36.577796Z",
     "start_time": "2022-10-14T06:43:36.574015Z"
    }
   },
   "outputs": [],
   "source": [
    "import getData as gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T06:43:36.890047Z",
     "start_time": "2022-10-14T06:43:36.887622Z"
    }
   },
   "outputs": [],
   "source": [
    "getDat = gd.getData(path_to_data=path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T06:43:49.862436Z",
     "start_time": "2022-10-14T06:43:37.338301Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get K-mer dictionary\n",
    "kmer_dct = getDat.get_unique_kmers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T06:44:36.176239Z",
     "start_time": "2022-10-14T06:44:36.143955Z"
    }
   },
   "outputs": [],
   "source": [
    "df = getDat.get_data(num_entries=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T06:44:39.595147Z",
     "start_time": "2022-10-14T06:44:39.395143Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>position</th>\n",
       "      <th>k-mer bases</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENST00000000233</td>\n",
       "      <td>244</td>\n",
       "      <td>AAGACCA</td>\n",
       "      <td>[[0.00299, 2.06, 125.0, 0.0177, 10.4, 122.0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENST00000000233</td>\n",
       "      <td>261</td>\n",
       "      <td>CAAACTG</td>\n",
       "      <td>[[0.0126, 1.95, 111.0, 0.0125, 1.27, 108.0, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENST00000000233</td>\n",
       "      <td>316</td>\n",
       "      <td>GAAACAG</td>\n",
       "      <td>[[0.00432, 2.02, 104.0, 0.00299, 3.56, 99.3, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENST00000000233</td>\n",
       "      <td>332</td>\n",
       "      <td>AGAACAT</td>\n",
       "      <td>[[0.0134, 4.71, 132.0, 0.00447, 4.24, 98.8, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENST00000000233</td>\n",
       "      <td>368</td>\n",
       "      <td>AGGACAA</td>\n",
       "      <td>[[0.015, 6.97, 118.0, 0.0106, 3.04, 123.0, 0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        transcript  position k-mer bases  \\\n",
       "0  ENST00000000233       244     AAGACCA   \n",
       "1  ENST00000000233       261     CAAACTG   \n",
       "2  ENST00000000233       316     GAAACAG   \n",
       "3  ENST00000000233       332     AGAACAT   \n",
       "4  ENST00000000233       368     AGGACAA   \n",
       "\n",
       "                                              values  \n",
       "0  [[0.00299, 2.06, 125.0, 0.0177, 10.4, 122.0, 0...  \n",
       "1  [[0.0126, 1.95, 111.0, 0.0125, 1.27, 108.0, 0....  \n",
       "2  [[0.00432, 2.02, 104.0, 0.00299, 3.56, 99.3, 0...  \n",
       "3  [[0.0134, 4.71, 132.0, 0.00447, 4.24, 98.8, 0....  \n",
       "4  [[0.015, 6.97, 118.0, 0.0106, 3.04, 123.0, 0.0...  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T06:02:04.183603Z",
     "start_time": "2022-10-14T06:02:03.640460Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T09:16:14.923865Z",
     "start_time": "2022-10-14T09:16:14.911345Z"
    }
   },
   "outputs": [],
   "source": [
    "def gen_one(fixed=False, idx=1):\n",
    "    q = lambda : random.randint(0, 66)\n",
    "    if not fixed:\n",
    "        return [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, q(), q(), q()]\n",
    "    else:\n",
    "        return [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, idx, idx+1, idx+2]\n",
    "        \n",
    "\n",
    "def gen_vect(size, fixed=False, idx = 1):\n",
    "    if not fixed:\n",
    "        return np.array([gen_one(idx = idx) for i in range(size)])\n",
    "    else :\n",
    "        return np.array([gen_one(True, idx = idx) for i in range(size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T09:28:43.602415Z",
     "start_time": "2022-10-14T09:28:43.590315Z"
    }
   },
   "outputs": [],
   "source": [
    "class m6aNet(nn.Module):\n",
    "    def __init__(self, batchsize, readsize):\n",
    "        self.batchsize = batchsize\n",
    "        self.readsize = readsize\n",
    "        super(m6aNet, self).__init__()\n",
    "        # Embedding Layer\n",
    "        self.embed = nn.Embedding(66, 2)\n",
    "\n",
    "        ## First Layer ##\n",
    "        self.read_level_prob_1 = nn.Linear(15, 150)\n",
    "        # First Batch Norm Layer\n",
    "        self.norm_1 = nn.BatchNorm1d(num_features=150)\n",
    "        # First Activation Layer\n",
    "        self.activ_1=nn.ReLU()\n",
    "\n",
    "        ## Second Layer ##\n",
    "        self.read_level_prob_2 = nn.Linear(150, 32)\n",
    "        # Second Activation Layer\n",
    "        self.activ_2=nn.ReLU()\n",
    "\n",
    "        ## Third Layer ##\n",
    "        self.read_level_prob_3 = nn.Linear(32, 1)\n",
    "        # Sigmoid Activation\n",
    "        self.sig_1 = nn.Sigmoid()\n",
    "\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        ### X is a numpy array of shape (batchsize, readsize=20, 12) ###\n",
    "        \n",
    "#         # Convert to tensor\n",
    "#         x = torch.tensor(x)\n",
    "        # Extract numeric features        \n",
    "        numerics = x[:, :, :9]\n",
    "        # # Extract Bases\n",
    "        bases = x[:, :, 9:].type(torch.int64)\n",
    "        # # Feed to embedding layer\n",
    "        bases = self.embed(bases)\n",
    "\n",
    "        # Reshape\n",
    "        bases = bases.reshape(self.batchsize, self.readsize, 3*2)\n",
    "        # Combine embedded output with numeric features\n",
    "        x = torch.concat((numerics, bases), 2).type(torch.float)\n",
    "\n",
    "        #### Feed Forward  ####\n",
    "\n",
    "        ## First Layer ##\n",
    "        x = self.read_level_prob_1(x)\n",
    "        # First Batch Norm Layer\n",
    "        x = x.transpose(dim0=1, dim1=2) # Need to transpose first\n",
    "        x = self.norm_1(x)\n",
    "        x = x.transpose(dim0=1, dim1=2) # Then transpose back\n",
    "        # First Activation Layer\n",
    "        x= self.activ_1(x)\n",
    "\n",
    "        ## Second Layer ##\n",
    "        x = self.read_level_prob_2(x)\n",
    "        # Second Activation Layer\n",
    "        x = self.activ_2(x)\n",
    "\n",
    "        ## Third Layer ##\n",
    "        x = self.read_level_prob_3(x)\n",
    "        # Sigmoid Activation\n",
    "        x = self.sig_1(x)\n",
    "        x = x.reshape(-1, self.readsize)\n",
    "\n",
    "        # Final Output\n",
    "        r = 1 - torch.prod(1 - x, axis=1)\n",
    "        return r\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T09:28:49.627001Z",
     "start_time": "2022-10-14T09:28:49.622059Z"
    }
   },
   "outputs": [],
   "source": [
    "model=m6aNet(batchsize=2, readsize=20)\n",
    "results = model.forward(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_epochs = 10\n",
    "# Automatically implements the MSE formula\n",
    "loss = nn.BCELoss()\n",
    "# Use Stochastic Gradient Descent. Need to supply the model parameters \n",
    "# and a selected learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "318.3px",
    "left": "0px",
    "top": "111.125px",
    "width": "196.35px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
